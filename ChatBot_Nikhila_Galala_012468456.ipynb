{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatBot using TensorFlow with RNN and LSTM\n",
    "\n",
    "### Steps followed:\n",
    "1. Import the data from conversation.txt file\n",
    "2. Process the data into a suitable format\n",
    "3. Build the model\n",
    "4. Train the model\n",
    "5. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"conversation.txt\", \"r\") as ins:\n",
    "    sentences = []\n",
    "    for line in ins:\n",
    "        sentences.append(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Preprocess the data\n",
    "The data is currently in a format that is not helpful for deep learning model. Hence the below steps are followed:\n",
    "\n",
    "- Create a list of questions whose corresponding answers.\n",
    "- Create buckets based on the length of the sentenses and tokens for padding\n",
    "- Convert these questions and answers into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "for j in range(512):\n",
    "    for i in range(len(sentences)):\n",
    "        if i%2 == 0:\n",
    "            questions.append(sentences[i])\n",
    "        else:\n",
    "            answers.append(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you\n",
      "\n",
      "I'm doing good\n",
      "\n",
      "\n",
      "what's your name\n",
      "\n",
      "I'm nameless\n",
      "\n",
      "\n",
      "what can you do\n",
      "\n",
      "anything that you want\n",
      "\n",
      "\n",
      "when can I check in\n",
      "\n",
      "the date specified on your reservation\n",
      "\n",
      "\n",
      "when can I check out\n",
      "\n",
      "whenever you want to\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limit = 5\n",
    "for i in range(limit, limit+5):\n",
    "    print(questions[i])\n",
    "    print(answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cleaning the text\n",
    "Clean the text. That is to remove punctuation and convert english word abbreviations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\",\"i am\", text)\n",
    "    text = re.sub(r\"he's\",\"he is\", text)\n",
    "    text = re.sub(r\"she's\",\"she is\", text)\n",
    "    text = re.sub(r\"that's\",\"that is\", text)\n",
    "    text = re.sub(r\"what's\",\"what is\", text)\n",
    "    text = re.sub(r\"where's\",\"where is\", text)\n",
    "    text = re.sub(r\"it's\",\"it is\", text)\n",
    "    text = re.sub(r\"\\'ll\",\" will\", text)\n",
    "    text = re.sub(r\"\\'ve\",\" have\", text)\n",
    "    text = re.sub(r\"\\'re\",\" are\", text)\n",
    "    text = re.sub(r\"\\'d\",\" would\", text)\n",
    "    text = re.sub(r\"won't\",\"will not\", text)\n",
    "    text = re.sub(r\"can't\",\"cannot\", text)\n",
    "    text = re.sub(r'[-()\\\"#/@;:<>{}+=~|.?,!]','',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question)) \n",
    "    \n",
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you\n",
      "\n",
      "i am doing good\n",
      "\n",
      "\n",
      "what is your name\n",
      "\n",
      "i am nameless\n",
      "\n",
      "\n",
      "what can you do\n",
      "\n",
      "anything that you want\n",
      "\n",
      "\n",
      "when can i check in\n",
      "\n",
      "the date specified on your reservation\n",
      "\n",
      "\n",
      "when can i check out\n",
      "\n",
      "whenever you want to\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limit = 5\n",
    "for i in range(limit, limit+5):\n",
    "    print(clean_questions[i])\n",
    "    print(clean_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "for question in clean_questions:\n",
    "    for word in question.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "\n",
    "for answer in clean_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "questionswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold:\n",
    "        questionswords2int[word] = word_number\n",
    "        word_number += 1\n",
    "        \n",
    "answerswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold:\n",
    "        answerswords2int[word] = word_number\n",
    "        word_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Adding tokens\n",
    "Now add the tokens to this dictionary. These will come in handy later on when we manipulate our strings further for the model.\n",
    "We use a few special symbols to fill in the sequence.\n",
    "1.\tEOS : End of sentence\n",
    "2.\tPAD : Filler\n",
    "3.\tSOS : Start decoding\n",
    "4.\tOUT : Unknown; word not in vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "\n",
    "for token in tokens:\n",
    "    questionswords2int[token] = len(questionswords2int) + 1\n",
    "    \n",
    "for token in tokens:\n",
    "    answerswords2int[token] = len(answerswords2int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add EOS at the end of each answer\n",
    "\n",
    "for i in range(len(clean_answers)):\n",
    "    clean_answers[i] += \" <EOS>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Translating the words to integers\n",
    "As stated at the beginning of this section, translate all of the words into integers that correspond to that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_into_int = []\n",
    "for question in clean_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questionswords2int:\n",
    "            ints.append(questionswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(questionswords2int[word])\n",
    "    questions_into_int.append(ints)\n",
    "answers_into_int = []\n",
    "for answer in clean_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answerswords2int:\n",
    "            ints.append(answerswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(answerswords2int[word])\n",
    "    answers_into_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 8, 9]\n",
      "[15, 61, 62, 63, 113]\n",
      "\n",
      "[3, 4, 10, 11]\n",
      "[15, 61, 64, 113]\n",
      "\n",
      "[3, 12, 9, 13]\n",
      "[65, 66, 9, 67, 113]\n",
      "\n",
      "[14, 12, 15, 16, 17]\n",
      "[48, 68, 69, 70, 10, 71, 113]\n",
      "\n",
      "[14, 12, 15, 16, 18]\n",
      "[72, 9, 67, 28, 113]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limit = 5\n",
    "for i in range(limit, limit+5):\n",
    "    print(questions_into_int[i])\n",
    "    print(answers_into_int[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_clean_questions = []\n",
    "sorted_clean_answers = []\n",
    "\n",
    "for length in range(1, 25 + 1):\n",
    "    for i in enumerate(questions_into_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
    "            sorted_clean_answers.append(answers_into_int[i[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3 Building the model\n",
    "The pre-processing of the data is now over. Now create a model to take this data and learn from it.\n",
    "\n",
    "Process for creating this model will be:\n",
    "1. Define model inputs\n",
    "2. Target preprocessing function\n",
    "3. Create the Encoder RNN layer\n",
    "4. Create Decoder RNN layer \n",
    "4. Define the Seq2Seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model inputs function\n",
    "Define the placeholders for model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
    "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    return inputs, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Preprocess targets\n",
    "This function will process  targets in a format for model to understand, including by adding SOS (Start of sentence) at the beginning of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_targets(targets, word2int, batch_size):\n",
    "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
    "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
    "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
    "    return preprocessed_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Creating the Encoder RNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
    "                                                                    cell_bw = encoder_cell,\n",
    "                                                                    sequence_length = sequence_length,\n",
    "                                                                    inputs = rnn_inputs,\n",
    "                                                                    dtype = tf.float32)\n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.4 Create the Decoder RNN Layer \n",
    "This part of the model is fairly involved as I need to split this section into 3 parts to create decoder RNN layer:\n",
    "1. Decode the training set\n",
    "2. Decode the validation set\n",
    "3. Decode the RNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding the training set\n",
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              name = \"attn_dec_train\")\n",
    "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                              training_decoder_function,\n",
    "                                                                                                              decoder_embedded_input,\n",
    "                                                                                                              sequence_length,\n",
    "                                                                                                              scope = decoding_scope)\n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    return output_function(decoder_output_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding the test/validation set\n",
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
    "                                                                              encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              decoder_embeddings_matrix,\n",
    "                                                                              sos_id,\n",
    "                                                                              eos_id,\n",
    "                                                                              maximum_length,\n",
    "                                                                              num_words,\n",
    "                                                                              name = \"attn_dec_inf\")\n",
    "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                                test_decoder_function,\n",
    "                                                                                                                scope = decoding_scope)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Decoder RNN\n",
    "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                      num_words,\n",
    "                                                                      None,\n",
    "                                                                      scope = decoding_scope,\n",
    "                                                                      weights_initializer = weights,\n",
    "                                                                      biases_initializer = biases)\n",
    "        training_predictions = decode_training_set(encoder_state,\n",
    "                                                   decoder_cell,\n",
    "                                                   decoder_embedded_input,\n",
    "                                                   sequence_length,\n",
    "                                                   decoding_scope,\n",
    "                                                   output_function,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        test_predictions = decode_test_set(encoder_state,\n",
    "                                           decoder_cell,\n",
    "                                           decoder_embeddings_matrix,\n",
    "                                           word2int['<SOS>'],\n",
    "                                           word2int['<EOS>'],\n",
    "                                           sequence_length - 1,\n",
    "                                           num_words,\n",
    "                                           decoding_scope,\n",
    "                                           output_function,\n",
    "                                           keep_prob,\n",
    "                                           batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Defining the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
    "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                              answers_num_words + 1,\n",
    "                                                              encoder_embedding_size,\n",
    "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
    "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
    "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
    "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
    "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
    "                                                         decoder_embeddings_matrix,\n",
    "                                                         encoder_state,\n",
    "                                                         questions_num_words,\n",
    "                                                         sequence_length,\n",
    "                                                         rnn_size,\n",
    "                                                         num_layers,\n",
    "                                                         questionswords2int,\n",
    "                                                         keep_prob,\n",
    "                                                         batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the model\n",
    "Our RNN model has been built, now it's time to train the model on data.\n",
    "\n",
    "Process for training the model will be as follows:\n",
    "1. Initial set-up\n",
    "2. Define loss error, optimizer etc.\n",
    "3. Further string manipulation\n",
    "4. Training for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initial set-up\n",
    "To set up model training the following are done:\n",
    "- Define hyperparameters\n",
    "- Define session\n",
    "- Load the model inputs\n",
    "- Set length of sequence and the shape of  inputs\n",
    "- Get training and test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Setting hyperparameters\n",
    "Training a deep learning model takes time. I had to make a pay-off between optimal chatbot performance and training time. To strike a good balance I decided on the following parameters which resulted in a training time of roughly 4 hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate= 0.0001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define session\n",
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model inputs\n",
    "inputs, targets, lr, keep_prob = model_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting sequence length\n",
    "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the shape of inputs tensor\n",
    "input_shape = tf.shape(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting training and test predictions\n",
    "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                       targets,\n",
    "                                                       keep_prob,\n",
    "                                                       batch_size,\n",
    "                                                       sequence_length,\n",
    "                                                       len(answerswords2int),\n",
    "                                                       len(questionswords2int),\n",
    "                                                       encoding_embedding_size,\n",
    "                                                       decoding_embedding_size,\n",
    "                                                       rnn_size,\n",
    "                                                       num_layers,\n",
    "                                                       questionswords2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Defining loss error, optimizer etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"optimization\"):\n",
    "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
    "                                                  targets,\n",
    "                                                  tf.ones([input_shape[0], sequence_length]))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss_error)\n",
    "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
    "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Further string manipulation\n",
    "Before entering the training for loop some further string manipulationa are done. Namely:\n",
    "\n",
    "- Add padding so each sentence is the same length\n",
    "- Split  data into the batches that will be fed into the model\n",
    "- Train, dev, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the sequences with PAD\n",
    "def apply_padding(batch_of_sequences, word2int):\n",
    "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
    "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into batches of questions and answers\n",
    "def split_into_batches(questions, answers, batch_size):\n",
    "    for batch_index in range(0, len(questions) // batch_size):\n",
    "        start_index = batch_index * batch_size\n",
    "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
    "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
    "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
    "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
    "        yield padded_questions_in_batch, padded_answers_in_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_shuffled, questions_shuffled_testing, answers_shuffled, answers_shuffled_testing = train_test_split(sorted_clean_questions,sorted_clean_answers,test_size=0.1,shuffle=True,random_state=101)\n",
    "\n",
    "\n",
    "training_validation_split = int(len(questions_shuffled) * 0.1)\n",
    "training_questions = questions_shuffled[training_validation_split:]\n",
    "training_answers = answers_shuffled[training_validation_split:]\n",
    "validation_questions = questions_shuffled[:training_validation_split]\n",
    "validation_answers = answers_shuffled[:training_validation_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Training for loop\n",
    "This is the section where the model will learn data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/10, Batch:    0/93, Training Loss Error:  0.048, Training Time on 100 Batches: 178 seconds\n",
      "Validation Loss Error:  1.237, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  0.876, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Epoch:   2/10, Batch:    0/93, Training Loss Error:  3.222, Training Time on 100 Batches: 176 seconds\n",
      "Validation Loss Error:  0.498, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  0.300, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Epoch:   3/10, Batch:    0/93, Training Loss Error:  0.609, Training Time on 100 Batches: 179 seconds\n",
      "Validation Loss Error:  0.212, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  0.189, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Epoch:   4/10, Batch:    0/93, Training Loss Error:  0.280, Training Time on 100 Batches: 177 seconds\n",
      "Validation Loss Error:  0.183, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  0.174, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Epoch:   5/10, Batch:    0/93, Training Loss Error:  0.211, Training Time on 100 Batches: 179 seconds\n",
      "Validation Loss Error:  0.174, Batch Validation Time: 5 seconds\n",
      "Sorry, I do not speak better, I need to get trained more.\n",
      "Validation Loss Error:  0.161, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Epoch:   6/10, Batch:    0/93, Training Loss Error:  0.193, Training Time on 100 Batches: 174 seconds\n",
      "Validation Loss Error:  0.157, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  0.148, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Epoch:   7/10, Batch:    0/93, Training Loss Error:  0.176, Training Time on 100 Batches: 177 seconds\n",
      "Validation Loss Error:  0.143, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  0.138, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Epoch:   8/10, Batch:    0/93, Training Loss Error:  0.161, Training Time on 100 Batches: 177 seconds\n",
      "Validation Loss Error:  0.128, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  0.121, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Epoch:   9/10, Batch:    0/93, Training Loss Error:  0.146, Training Time on 100 Batches: 176 seconds\n",
      "Validation Loss Error:  0.118, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Validation Loss Error:  0.113, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Epoch:  10/10, Batch:    0/93, Training Loss Error:  0.135, Training Time on 100 Batches: 180 seconds\n",
      "Validation Loss Error:  0.113, Batch Validation Time: 5 seconds\n",
      "Sorry, I do not speak better, I need to get trained more.\n",
      "Validation Loss Error:  0.111, Batch Validation Time: 5 seconds\n",
      "I speak better now!!\n",
      "Game Over\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "batch_index_check_training_loss = 100\n",
    "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
    "total_training_loss_error = 0\n",
    "list_validation_loss_error = []\n",
    "early_stopping_check = 0\n",
    "early_stopping_stop = 100\n",
    "checkpoint = \"chatbot_weights.ckpt\"\n",
    "session.run(tf.global_variables_initializer())\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
    "        starting_time = time.time()\n",
    "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
    "                                                                                               targets: padded_answers_in_batch,\n",
    "                                                                                               lr: learning_rate,\n",
    "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                                               keep_prob: keep_probability})\n",
    "        total_training_loss_error += batch_training_loss_error\n",
    "        ending_time = time.time()\n",
    "        batch_time = ending_time - starting_time\n",
    "        if batch_index % batch_index_check_training_loss == 0:\n",
    "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
    "                                                                                                                                       epochs,\n",
    "                                                                                                                                       batch_index,\n",
    "                                                                                                                                       len(training_questions) // batch_size,\n",
    "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
    "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
    "            total_training_loss_error = 0\n",
    "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
    "            total_validation_loss_error = 0\n",
    "            starting_time = time.time()\n",
    "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
    "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
    "                                                                       targets: padded_answers_in_batch,\n",
    "                                                                       lr: learning_rate,\n",
    "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                       keep_prob: 1})\n",
    "                total_validation_loss_error += batch_validation_loss_error\n",
    "            ending_time = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
    "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            list_validation_loss_error.append(average_validation_loss_error)\n",
    "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                print('I speak better now!!')\n",
    "                early_stopping_check = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, checkpoint)\n",
    "            else:\n",
    "                print(\"Sorry, I do not speak better, I need to get trained more.\")\n",
    "                early_stopping_check += 1\n",
    "                if early_stopping_check == early_stopping_stop:\n",
    "                    break\n",
    "    if early_stopping_check == early_stopping_stop:\n",
    "        print(\"I cannot speak better anymore. This is the best I can do.\")\n",
    "        break\n",
    "print(\"Game Over\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing chatbot\n",
    " model has now run through the training and is ready to be implemented. This is a short section and will go through this process:\n",
    "\n",
    "1. Begin the session\n",
    "2. Convert question to integers\n",
    "3. Setting up the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the weights and Running the session\n",
    "checkpoint = \"./chatbot_weights.ckpt\"\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the questions to integers\n",
    "def convert_string2int(question, word2int):\n",
    "    question = clean_text(question)\n",
    "    return [word2int.get(word, word2int['<OUT>']) for word in question.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hi\n",
      "ChatBot:  hi.\n",
      "You: how are you\n",
      "ChatBot:  I am doing good.\n",
      "You: who are you\n",
      "ChatBot:  I am nameless.\n",
      "You: what can you do\n",
      "ChatBot:  I don't have phone number either.\n",
      "You: what is your address\n",
      "ChatBot:  I don't have an address.\n",
      "You: when can I check out\n",
      "ChatBot:  I don't have phone number either.\n",
      "You: can you call someone for me\n",
      "ChatBot:  sure I am getting you someone to talk with you.\n",
      "You: I need to make an appointment\n",
      "ChatBot:  sure about what.\n",
      "You: I need to see the doctor\n",
      "ChatBot:  sure about what.\n",
      "You: My child needs to come in for a check-up\n",
      "ChatBot:  can you come to the clinic.\n",
      "You: When is the doctor free\n",
      "ChatBot:  sure for what.\n",
      "You: how to get to your office if I come from San Jose\n",
      "ChatBot:  you need to take the vta.\n",
      "You: how to get to your office if I come by car\n",
      "ChatBot:  you need to take the vta.\n",
      "You: how to get to your office if I come by motorbike\n",
      "ChatBot:  you need to take the vta.\n",
      "You: GoodBye\n",
      "ChatBot:  have a nice day.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the chat\n",
    "i=0\n",
    "while(True):\n",
    "    question = input(\"You: \")\n",
    "    if question == 'Goodbye':\n",
    "        break\n",
    "    question = convert_string2int(question, questionswords2int)\n",
    "    question = question + [questionswords2int['<PAD>']] * (12 - len(question))\n",
    "    fake_batch = np.zeros((batch_size, 12))\n",
    "    fake_batch[0] = question\n",
    "    \n",
    "    predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.7})[0]\n",
    "    answer = ''\n",
    "    for i in np.argmax(predicted_answer, 1):\n",
    "        if answersints2word[i] == 'i':\n",
    "            token = ' I'\n",
    "        elif answersints2word[i] == '<EOS>':\n",
    "            token = '.'\n",
    "        elif answersints2word[i] == '<OUT>':\n",
    "            token = 'out'\n",
    "        else:\n",
    "            token = ' ' + answersints2word[i]\n",
    "        answer += token\n",
    "        if token == '.':\n",
    "            break\n",
    "    print('ChatBot: ' + answer)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
